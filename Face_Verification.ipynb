{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Face Verification System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FaceNet Face Recognition Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jn-Om0_Sv5M1"
      },
      "outputs": [],
      "source": [
        "#### PART OF THIS CODE IS USING CODE FROM VICTOR SY WANG: https://github.com/iwantooxxoox/Keras-OpenFace/blob/master/utils.py ####\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from numpy import genfromtxt\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "_FLOATX = 'float32'\n",
        "\n",
        "def variable(value, dtype=_FLOATX, name=None):\n",
        "    v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n",
        "    _get_session().run(v.initializer)\n",
        "    return v\n",
        "\n",
        "def shape(x):\n",
        "    return x.get_shape()\n",
        "\n",
        "def square(x):\n",
        "    return tf.square(x)\n",
        "\n",
        "def zeros(shape, dtype=_FLOATX, name=None):\n",
        "    return variable(np.zeros(shape), dtype, name)\n",
        "\n",
        "def concatenate(tensors, axis=-1):\n",
        "    if axis < 0:\n",
        "        axis = axis % len(tensors[0].get_shape())\n",
        "    return tf.concat(axis, tensors)\n",
        "\n",
        "def LRN2D(x):\n",
        "    return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n",
        "\n",
        "def conv2d_bn(x,\n",
        "              layer=None,\n",
        "              cv1_out=None,\n",
        "              cv1_filter=(1, 1),\n",
        "              cv1_strides=(1, 1),\n",
        "              cv2_out=None,\n",
        "              cv2_filter=(3, 3),\n",
        "              cv2_strides=(1, 1),\n",
        "              padding=None):\n",
        "    num = '' if cv2_out == None else '1'\n",
        "    tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, data_format='channels_first', name=layer+'_conv'+num)(x)\n",
        "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n",
        "    tensor = Activation('relu')(tensor)\n",
        "    if padding == None:\n",
        "        return tensor\n",
        "    tensor = ZeroPadding2D(padding=padding, data_format='channels_first')(tensor)\n",
        "    if cv2_out == None:\n",
        "        return tensor\n",
        "    tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, data_format='channels_first', name=layer+'_conv'+'2')(tensor)\n",
        "    tensor = BatchNormalization(axis=1, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n",
        "    tensor = Activation('relu')(tensor)\n",
        "    return tensor\n",
        "\n",
        "WEIGHTS = [\n",
        "  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3',\n",
        "  'inception_3a_1x1_conv', 'inception_3a_1x1_bn',\n",
        "  'inception_3a_pool_conv', 'inception_3a_pool_bn',\n",
        "  'inception_3a_5x5_conv1', 'inception_3a_5x5_conv2', 'inception_3a_5x5_bn1', 'inception_3a_5x5_bn2',\n",
        "  'inception_3a_3x3_conv1', 'inception_3a_3x3_conv2', 'inception_3a_3x3_bn1', 'inception_3a_3x3_bn2',\n",
        "  'inception_3b_3x3_conv1', 'inception_3b_3x3_conv2', 'inception_3b_3x3_bn1', 'inception_3b_3x3_bn2',\n",
        "  'inception_3b_5x5_conv1', 'inception_3b_5x5_conv2', 'inception_3b_5x5_bn1', 'inception_3b_5x5_bn2',\n",
        "  'inception_3b_pool_conv', 'inception_3b_pool_bn',\n",
        "  'inception_3b_1x1_conv', 'inception_3b_1x1_bn',\n",
        "  'inception_3c_3x3_conv1', 'inception_3c_3x3_conv2', 'inception_3c_3x3_bn1', 'inception_3c_3x3_bn2',\n",
        "  'inception_3c_5x5_conv1', 'inception_3c_5x5_conv2', 'inception_3c_5x5_bn1', 'inception_3c_5x5_bn2',\n",
        "  'inception_4a_3x3_conv1', 'inception_4a_3x3_conv2', 'inception_4a_3x3_bn1', 'inception_4a_3x3_bn2',\n",
        "  'inception_4a_5x5_conv1', 'inception_4a_5x5_conv2', 'inception_4a_5x5_bn1', 'inception_4a_5x5_bn2',\n",
        "  'inception_4a_pool_conv', 'inception_4a_pool_bn',\n",
        "  'inception_4a_1x1_conv', 'inception_4a_1x1_bn',\n",
        "  'inception_4e_3x3_conv1', 'inception_4e_3x3_conv2', 'inception_4e_3x3_bn1', 'inception_4e_3x3_bn2',\n",
        "  'inception_4e_5x5_conv1', 'inception_4e_5x5_conv2', 'inception_4e_5x5_bn1', 'inception_4e_5x5_bn2',\n",
        "  'inception_5a_3x3_conv1', 'inception_5a_3x3_conv2', 'inception_5a_3x3_bn1', 'inception_5a_3x3_bn2',\n",
        "  'inception_5a_pool_conv', 'inception_5a_pool_bn',\n",
        "  'inception_5a_1x1_conv', 'inception_5a_1x1_bn',\n",
        "  'inception_5b_3x3_conv1', 'inception_5b_3x3_conv2', 'inception_5b_3x3_bn1', 'inception_5b_3x3_bn2',\n",
        "  'inception_5b_pool_conv', 'inception_5b_pool_bn',\n",
        "  'inception_5b_1x1_conv', 'inception_5b_1x1_bn',\n",
        "  'dense_layer'\n",
        "]\n",
        "\n",
        "conv_shape = {\n",
        "  'conv1': [64, 3, 7, 7],\n",
        "  'conv2': [64, 64, 1, 1],\n",
        "  'conv3': [192, 64, 3, 3],\n",
        "  'inception_3a_1x1_conv': [64, 192, 1, 1],\n",
        "  'inception_3a_pool_conv': [32, 192, 1, 1],\n",
        "  'inception_3a_5x5_conv1': [16, 192, 1, 1],\n",
        "  'inception_3a_5x5_conv2': [32, 16, 5, 5],\n",
        "  'inception_3a_3x3_conv1': [96, 192, 1, 1],\n",
        "  'inception_3a_3x3_conv2': [128, 96, 3, 3],\n",
        "  'inception_3b_3x3_conv1': [96, 256, 1, 1],\n",
        "  'inception_3b_3x3_conv2': [128, 96, 3, 3],\n",
        "  'inception_3b_5x5_conv1': [32, 256, 1, 1],\n",
        "  'inception_3b_5x5_conv2': [64, 32, 5, 5],\n",
        "  'inception_3b_pool_conv': [64, 256, 1, 1],\n",
        "  'inception_3b_1x1_conv': [64, 256, 1, 1],\n",
        "  'inception_3c_3x3_conv1': [128, 320, 1, 1],\n",
        "  'inception_3c_3x3_conv2': [256, 128, 3, 3],\n",
        "  'inception_3c_5x5_conv1': [32, 320, 1, 1],\n",
        "  'inception_3c_5x5_conv2': [64, 32, 5, 5],\n",
        "  'inception_4a_3x3_conv1': [96, 640, 1, 1],\n",
        "  'inception_4a_3x3_conv2': [192, 96, 3, 3],\n",
        "  'inception_4a_5x5_conv1': [32, 640, 1, 1,],\n",
        "  'inception_4a_5x5_conv2': [64, 32, 5, 5],\n",
        "  'inception_4a_pool_conv': [128, 640, 1, 1],\n",
        "  'inception_4a_1x1_conv': [256, 640, 1, 1],\n",
        "  'inception_4e_3x3_conv1': [160, 640, 1, 1],\n",
        "  'inception_4e_3x3_conv2': [256, 160, 3, 3],\n",
        "  'inception_4e_5x5_conv1': [64, 640, 1, 1],\n",
        "  'inception_4e_5x5_conv2': [128, 64, 5, 5],\n",
        "  'inception_5a_3x3_conv1': [96, 1024, 1, 1],\n",
        "  'inception_5a_3x3_conv2': [384, 96, 3, 3],\n",
        "  'inception_5a_pool_conv': [96, 1024, 1, 1],\n",
        "  'inception_5a_1x1_conv': [256, 1024, 1, 1],\n",
        "  'inception_5b_3x3_conv1': [96, 736, 1, 1],\n",
        "  'inception_5b_3x3_conv2': [384, 96, 3, 3],\n",
        "  'inception_5b_pool_conv': [96, 736, 1, 1],\n",
        "  'inception_5b_1x1_conv': [256, 736, 1, 1],\n",
        "}\n",
        "\n",
        "def load_weights_from_FaceNet(FRmodel):\n",
        "    # Load weights from csv files (which was exported from Openface torch model)\n",
        "    weights = WEIGHTS\n",
        "    weights_dict = load_weights()\n",
        "\n",
        "    # Set layer weights of the model\n",
        "    for name in weights:\n",
        "        if FRmodel.get_layer(name) != None:\n",
        "            FRmodel.get_layer(name).set_weights(weights_dict[name])\n",
        "        elif model.get_layer(name) != None:\n",
        "            model.get_layer(name).set_weights(weights_dict[name])\n",
        "\n",
        "def load_weights():\n",
        "    # Set weights path\n",
        "    dirPath = '/content/weights/'\n",
        "    fileNames = filter(lambda f: not f.startswith('.'), os.listdir(dirPath))\n",
        "    paths = {}\n",
        "    weights_dict = {}\n",
        "\n",
        "    for n in fileNames:\n",
        "        paths[n.replace('.csv', '')] = dirPath + '/' + n\n",
        "\n",
        "    for name in WEIGHTS:\n",
        "        if 'conv' in name:\n",
        "            conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
        "            conv_w = np.reshape(conv_w, conv_shape[name])\n",
        "            conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n",
        "            conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
        "            weights_dict[name] = [conv_w, conv_b]     \n",
        "        elif 'bn' in name:\n",
        "            bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
        "            bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
        "            bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n",
        "            bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n",
        "            weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n",
        "        elif 'dense' in name:\n",
        "            dense_w = genfromtxt(dirPath+'/dense_w.csv', delimiter=',', dtype=None)\n",
        "            dense_w = np.reshape(dense_w, (128, 736))\n",
        "            dense_w = np.transpose(dense_w, (1, 0))\n",
        "            dense_b = genfromtxt(dirPath+'/dense_b.csv', delimiter=',', dtype=None)\n",
        "            weights_dict[name] = [dense_w, dense_b]\n",
        "\n",
        "    return weights_dict\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    train_dataset = h5py.File('datasets/train_happy.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('datasets/test_happy.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "    \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "    \n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
        "\n",
        "def img_to_encoding(image_path, model):\n",
        "    img1 = cv2.imread(image_path, 1)\n",
        "    img = img1[...,::-1]\n",
        "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
        "    x_train = np.array([img])\n",
        "    embedding = model.predict_on_batch(x_train)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inception Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5Uiz0zI6v3pV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from numpy import genfromtxt\n",
        "from keras import backend as K\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "\n",
        "def inception_block_1a(X):\n",
        "    \"\"\"\n",
        "    Implementation of an inception block\n",
        "    \"\"\"\n",
        "    \n",
        "    X_3x3 = Conv2D(96, (1, 1), data_format='channels_first', name ='inception_3a_3x3_conv1')(X)\n",
        "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name = 'inception_3a_3x3_bn1')(X_3x3)\n",
        "    X_3x3 = Activation('relu')(X_3x3)\n",
        "    X_3x3 = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_3x3)\n",
        "    X_3x3 = Conv2D(128, (3, 3), data_format='channels_first', name='inception_3a_3x3_conv2')(X_3x3)\n",
        "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_3x3_bn2')(X_3x3)\n",
        "    X_3x3 = Activation('relu')(X_3x3)\n",
        "    \n",
        "    X_5x5 = Conv2D(16, (1, 1), data_format='channels_first', name='inception_3a_5x5_conv1')(X)\n",
        "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_5x5_bn1')(X_5x5)\n",
        "    X_5x5 = Activation('relu')(X_5x5)\n",
        "    X_5x5 = ZeroPadding2D(padding=(2, 2), data_format='channels_first')(X_5x5)\n",
        "    X_5x5 = Conv2D(32, (5, 5), data_format='channels_first', name='inception_3a_5x5_conv2')(X_5x5)\n",
        "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_5x5_bn2')(X_5x5)\n",
        "    X_5x5 = Activation('relu')(X_5x5)\n",
        "\n",
        "    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n",
        "    X_pool = Conv2D(32, (1, 1), data_format='channels_first', name='inception_3a_pool_conv')(X_pool)\n",
        "    X_pool = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_pool_bn')(X_pool)\n",
        "    X_pool = Activation('relu')(X_pool)\n",
        "    X_pool = ZeroPadding2D(padding=((3, 4), (3, 4)), data_format='channels_first')(X_pool)\n",
        "\n",
        "    X_1x1 = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3a_1x1_conv')(X)\n",
        "    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_1x1_bn')(X_1x1)\n",
        "    X_1x1 = Activation('relu')(X_1x1)\n",
        "        \n",
        "    # CONCAT\n",
        "    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n",
        "\n",
        "    return inception\n",
        "\n",
        "def inception_block_1b(X):\n",
        "    X_3x3 = Conv2D(96, (1, 1), data_format='channels_first', name='inception_3b_3x3_conv1')(X)\n",
        "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_3x3_bn1')(X_3x3)\n",
        "    X_3x3 = Activation('relu')(X_3x3)\n",
        "    X_3x3 = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_3x3)\n",
        "    X_3x3 = Conv2D(128, (3, 3), data_format='channels_first', name='inception_3b_3x3_conv2')(X_3x3)\n",
        "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_3x3_bn2')(X_3x3)\n",
        "    X_3x3 = Activation('relu')(X_3x3)\n",
        "\n",
        "    X_5x5 = Conv2D(32, (1, 1), data_format='channels_first', name='inception_3b_5x5_conv1')(X)\n",
        "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_5x5_bn1')(X_5x5)\n",
        "    X_5x5 = Activation('relu')(X_5x5)\n",
        "    X_5x5 = ZeroPadding2D(padding=(2, 2), data_format='channels_first')(X_5x5)\n",
        "    X_5x5 = Conv2D(64, (5, 5), data_format='channels_first', name='inception_3b_5x5_conv2')(X_5x5)\n",
        "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_5x5_bn2')(X_5x5)\n",
        "    X_5x5 = Activation('relu')(X_5x5)\n",
        "\n",
        "    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n",
        "    X_pool = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3b_pool_conv')(X_pool)\n",
        "    X_pool = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_pool_bn')(X_pool)\n",
        "    X_pool = Activation('relu')(X_pool)\n",
        "    X_pool = ZeroPadding2D(padding=(4, 4), data_format='channels_first')(X_pool)\n",
        "\n",
        "    X_1x1 = Conv2D(64, (1, 1), data_format='channels_first', name='inception_3b_1x1_conv')(X)\n",
        "    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3b_1x1_bn')(X_1x1)\n",
        "    X_1x1 = Activation('relu')(X_1x1)\n",
        "\n",
        "    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n",
        "\n",
        "    return inception\n",
        "\n",
        "def inception_block_1c(X):\n",
        "    X_3x3 = conv2d_bn(X,\n",
        "                           layer='inception_3c_3x3',\n",
        "                           cv1_out=128,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=256,\n",
        "                           cv2_filter=(3, 3),\n",
        "                           cv2_strides=(2, 2),\n",
        "                           padding=(1, 1))\n",
        "\n",
        "    X_5x5 = conv2d_bn(X,\n",
        "                           layer='inception_3c_5x5',\n",
        "                           cv1_out=32,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=64,\n",
        "                           cv2_filter=(5, 5),\n",
        "                           cv2_strides=(2, 2),\n",
        "                           padding=(2, 2))\n",
        "\n",
        "    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n",
        "    X_pool = ZeroPadding2D(padding=((0, 1), (0, 1)), data_format='channels_first')(X_pool)\n",
        "\n",
        "    inception = concatenate([X_3x3, X_5x5, X_pool], axis=1)\n",
        "\n",
        "    return inception\n",
        "\n",
        "def inception_block_2a(X):\n",
        "    X_3x3 = conv2d_bn(X,\n",
        "                           layer='inception_4a_3x3',\n",
        "                           cv1_out=96,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=192,\n",
        "                           cv2_filter=(3, 3),\n",
        "                           cv2_strides=(1, 1),\n",
        "                           padding=(1, 1))\n",
        "    X_5x5 = conv2d_bn(X,\n",
        "                           layer='inception_4a_5x5',\n",
        "                           cv1_out=32,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=64,\n",
        "                           cv2_filter=(5, 5),\n",
        "                           cv2_strides=(1, 1),\n",
        "                           padding=(2, 2))\n",
        "\n",
        "    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n",
        "    X_pool = conv2d_bn(X_pool,\n",
        "                           layer='inception_4a_pool',\n",
        "                           cv1_out=128,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           padding=(2, 2))\n",
        "    X_1x1 = conv2d_bn(X,\n",
        "                           layer='inception_4a_1x1',\n",
        "                           cv1_out=256,\n",
        "                           cv1_filter=(1, 1))\n",
        "    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=1)\n",
        "\n",
        "    return inception\n",
        "\n",
        "def inception_block_2b(X):\n",
        "    #inception4e\n",
        "    X_3x3 = conv2d_bn(X,\n",
        "                           layer='inception_4e_3x3',\n",
        "                           cv1_out=160,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=256,\n",
        "                           cv2_filter=(3, 3),\n",
        "                           cv2_strides=(2, 2),\n",
        "                           padding=(1, 1))\n",
        "    X_5x5 = conv2d_bn(X,\n",
        "                           layer='inception_4e_5x5',\n",
        "                           cv1_out=64,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=128,\n",
        "                           cv2_filter=(5, 5),\n",
        "                           cv2_strides=(2, 2),\n",
        "                           padding=(2, 2))\n",
        "    \n",
        "    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n",
        "    X_pool = ZeroPadding2D(padding=((0, 1), (0, 1)), data_format='channels_first')(X_pool)\n",
        "\n",
        "    inception = concatenate([X_3x3, X_5x5, X_pool], axis=1)\n",
        "\n",
        "    return inception\n",
        "\n",
        "def inception_block_3a(X):\n",
        "    X_3x3 = conv2d_bn(X,\n",
        "                           layer='inception_5a_3x3',\n",
        "                           cv1_out=96,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=384,\n",
        "                           cv2_filter=(3, 3),\n",
        "                           cv2_strides=(1, 1),\n",
        "                           padding=(1, 1))\n",
        "    X_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3), data_format='channels_first')(X)\n",
        "    X_pool = conv2d_bn(X_pool,\n",
        "                           layer='inception_5a_pool',\n",
        "                           cv1_out=96,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           padding=(1, 1))\n",
        "    X_1x1 = conv2d_bn(X,\n",
        "                           layer='inception_5a_1x1',\n",
        "                           cv1_out=256,\n",
        "                           cv1_filter=(1, 1))\n",
        "\n",
        "    inception = concatenate([X_3x3, X_pool, X_1x1], axis=1)\n",
        "\n",
        "    return inception\n",
        "\n",
        "def inception_block_3b(X):\n",
        "    X_3x3 = conv2d_bn(X,\n",
        "                           layer='inception_5b_3x3',\n",
        "                           cv1_out=96,\n",
        "                           cv1_filter=(1, 1),\n",
        "                           cv2_out=384,\n",
        "                           cv2_filter=(3, 3),\n",
        "                           cv2_strides=(1, 1),\n",
        "                           padding=(1, 1))\n",
        "    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n",
        "    X_pool = conv2d_bn(X_pool,\n",
        "                           layer='inception_5b_pool',\n",
        "                           cv1_out=96,\n",
        "                           cv1_filter=(1, 1))\n",
        "    X_pool = ZeroPadding2D(padding=(1, 1), data_format='channels_first')(X_pool)\n",
        "\n",
        "    X_1x1 = conv2d_bn(X,\n",
        "                           layer='inception_5b_1x1',\n",
        "                           cv1_out=256,\n",
        "                           cv1_filter=(1, 1))\n",
        "    inception = concatenate([X_3x3, X_pool, X_1x1], axis=1)\n",
        "\n",
        "    return inception\n",
        "\n",
        "def faceRecoModel(input_shape):\n",
        "    \"\"\"\n",
        "    Implementation of the Inception model used for FaceNet\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "        \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # First Block\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n",
        "    X = BatchNormalization(axis = 1, name = 'bn1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Zero-Padding + MAXPOOL\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "    X = MaxPooling2D((3, 3), strides = 2)(X)\n",
        "    \n",
        "    # Second Block\n",
        "    X = Conv2D(64, (1, 1), strides = (1, 1), name = 'conv2')(X)\n",
        "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn2')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Zero-Padding + MAXPOOL\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "\n",
        "    # Second Block\n",
        "    X = Conv2D(192, (3, 3), strides = (1, 1), name = 'conv3')(X)\n",
        "    X = BatchNormalization(axis = 1, epsilon=0.00001, name = 'bn3')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Zero-Padding + MAXPOOL\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "    X = MaxPooling2D(pool_size = 3, strides = 2)(X)\n",
        "    \n",
        "    # Inception 1: a/b/c\n",
        "    X = inception_block_1a(X)\n",
        "    X = inception_block_1b(X)\n",
        "    X = inception_block_1c(X)\n",
        "    \n",
        "    # Inception 2: a/b\n",
        "    X = inception_block_2a(X)\n",
        "    X = inception_block_2b(X)\n",
        "    \n",
        "    # Inception 3: a/b\n",
        "    X = inception_block_3a(X)\n",
        "    X = inception_block_3b(X)\n",
        "    \n",
        "    # Top layer\n",
        "    X = AveragePooling2D(pool_size=(3, 3), strides=(1, 1), data_format='channels_first')(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(128, name='dense_layer')(X)\n",
        "    \n",
        "    # L2 normalization\n",
        "    X = Lambda(lambda  x: K.l2_normalize(x,axis=1))(X)\n",
        "\n",
        "    # Create model instance\n",
        "    model = Model(inputs = X_input, outputs = X, name='FaceRecoModel')\n",
        "        \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vHpjlbemuI3y"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.layers import Layer\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download Pretrained FaceNet Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v2biAtO12wVI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "if not os.path.exists('/content/weights'):\n",
        "  os.mkdir('/content/weights')\n",
        "  \n",
        "exp = re.compile('\\.csv$')\n",
        "wp = requests.get(\"https://github.com/csaybar/DLcoursera/tree/master/Convolutional%20Neural%20Networks/week04/Face%20Recognition/weights\")\n",
        "soup = BeautifulSoup(wp.content, 'html.parser')\n",
        "\n",
        "images = []\n",
        "for element in soup.find_all('a',class_=\"js-navigation-open\"):\n",
        "  filename = exp.search(element['href'])\n",
        "  if bool(filename):\n",
        "    image_url = 'https://raw.githubusercontent.com' + element['href'].replace('/blob', '')\n",
        "    urllib.request.urlretrieve(image_url, '/content/weights/' + os.path.basename(image_url))\n",
        "    images.append(image_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Recognition Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "96x96 RGB Images to 128 dimensional image encoding vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6m2ESh_7uI38"
      },
      "outputs": [],
      "source": [
        "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Triplet Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jE8Mw_zQuI4F"
      },
      "outputs": [],
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    \"\"\"\n",
        "    Implementation of the triplet loss as defined by formula (3)\n",
        "    \n",
        "    Arguments:\n",
        "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
        "    y_pred -- python list containing three objects:\n",
        "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
        "            positive -- the encodings for the positive images, of shape (None, 128)\n",
        "            negative -- the encodings for the negative images, of shape (None, 128)\n",
        "    \n",
        "    Returns:\n",
        "    loss -- real number, value of the loss\n",
        "    \"\"\"\n",
        "    \n",
        "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1, keepdims=True)\n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1, keepdims=True)\n",
        "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
        "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Pre-Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nDSn0LdOuI4N"
      },
      "outputs": [],
      "source": [
        "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
        "load_weights_from_FaceNet(FRmodel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Pre-processing (Face Extraction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BacryENH6SC6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def extract_face(f, required_size=(96, 96)):\n",
        "  # Define paths\n",
        "  prototxt_path = '/content/drive/MyDrive/Stanford/236/model_data/deploy.prototxt.txt'\n",
        "  caffemodel_path = '/content/drive/MyDrive/Stanford/236/model_data/weights.caffemodel'\n",
        "\n",
        "  # Read the model\n",
        "  model = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n",
        "\n",
        "  # Create directory 'faces' if it does not exist\n",
        "  if not os.path.exists('faces'):\n",
        "\t  print(\"New directory created\")\n",
        "\t  os.makedirs('faces')\n",
        "\n",
        "  # Loop through all images and strip out faces\n",
        "  count = 0\n",
        "  file = f[6:]\n",
        "  #f = \"images/\"+file\n",
        "\n",
        "  file_name, file_extension = os.path.splitext(f)\n",
        "  if (file_extension in ['.png','.jpg']):\n",
        "    image = cv2.imread(f)\n",
        "    (h, w) = image.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (200, 200), interpolation = cv2.INTER_AREA), 1.0, (200, 200), (104.0, 177.0, 123.0))\n",
        "    model.setInput(blob)\n",
        "    detections = model.forward()\n",
        "\n",
        "\t\t# Identify each face\n",
        "    for i in range(0, 1):#detections.shape[2]):\n",
        "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "      (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "      confidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t\t# If confidence > 0.5, save it as a separate file\n",
        "      if (confidence > 0.5):\n",
        "        count += 1\n",
        "        frame = image[startY:endY, startX:endX]\n",
        "        if (frame.shape[0]<=75 or frame.shape[1]<=75):\n",
        "          return False\n",
        "        frame = cv2.resize(frame, required_size, interpolation = cv2.INTER_AREA)\n",
        "        cv2.imwrite('faces/'+str(i) + '_' + file, frame)\n",
        "      else:\n",
        "        return False\n",
        "  return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KZZG2lUT-Vw",
        "outputId": "791627c9-2951-4d00-e9e3-65c49e6978b7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip \"/content/drive/MyDrive/Stanford/236/236 Dataset/fairface-img-margin025-trainval.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Do not run this function - takes very long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zaw9TA5kVh3i",
        "outputId": "a0a0e3b1-ef38-4846-b5b9-7e907241a5a6"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "path = \"train/*.*\"\n",
        "database = {}\n",
        "for file in glob.glob(path):\n",
        "  f = file[6:-4]\n",
        "  if extract_face(file):\n",
        "    database[f] = img_to_encoding(\"faces/0_\"+file[6:], FRmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle \n",
        "with open('database.pkl', 'wb') as f:\n",
        "    pickle.dump(database, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run this block to load pre-created database of image encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('database.pkl', 'rb') as f:\n",
        "    database1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add user image to database for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4wreH5nlNoT7"
      },
      "outputs": [],
      "source": [
        "extract_face(\"train/Snapchat-673047922.jpg\")\n",
        "database[\"sanjana\"] = img_to_encoding(\"faces/0_Snapchat-673047922.jpg\", FRmodel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build Verification Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Distance based on Frobenius Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-U-TRsHtuI4X"
      },
      "outputs": [],
      "source": [
        "def verify(image_path, identity, database, model):\n",
        "    \"\"\"\n",
        "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
        "    \n",
        "    Arguments:\n",
        "    image_path -- path to an image\n",
        "    identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house.\n",
        "    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
        "    model -- your Inception model instance in Keras\n",
        "    \n",
        "    Returns:\n",
        "    dist -- distance between the image_path and the image of \"identity\" in the database.\n",
        "    door_open -- True, if the door should open. False otherwise.\n",
        "    \"\"\"\n",
        "  \n",
        "    encoding = img_to_encoding(image_path, model)\n",
        "    dist = np.linalg.norm(encoding - database[identity])\n",
        "    if dist < 0.7:\n",
        "        door_open = True\n",
        "    else:\n",
        "        door_open = False\n",
        "        \n",
        "    return dist, door_open"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mahalanobis Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "def distance (image_path, vals, model):\n",
        "    encoding = img_to_encoding(image_path, model)\n",
        "    features_mean = np.mean(vals, axis=0)\n",
        "    features_covmat = np.cov(vals.T)\n",
        "    features_covmat_inv = scipy.linalg.inv(features_covmat)\n",
        "    print (features_covmat_inv.shape)\n",
        "    return scipy.spatial.distance.mahalanobis(encoding, features_mean, features_covmat_inv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdK4xyoWL7QW",
        "outputId": "ae14a943-9ae1-40e4-8020-b3ee9546b704"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.5965422, True)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_face(\"train/link1.jpg\")\n",
        "verify(\"faces/0_link1.jpg\", \"sanjana\", database, FRmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5HDXSHDIuI4u",
        "outputId": "d4596a64-670c-4d0e-dc7b-ea80baf9e26b",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10174\n",
            "38655\n",
            "32398\n",
            "76697\n",
            "19543\n",
            "70843\n",
            "45647\n",
            "66824\n",
            "52138\n",
            "10044\n",
            "23598\n",
            "26501\n",
            "26972\n",
            "75802\n",
            "55274\n",
            "48323\n",
            "14109\n",
            "45980\n",
            "58961\n",
            "4531\n",
            "41801\n",
            "53184\n",
            "65981\n",
            "40773\n",
            "834\n",
            "46887\n",
            "69527\n",
            "72088\n",
            "25255\n",
            "10589\n",
            "42880\n",
            "6392\n",
            "14854\n",
            "12573\n",
            "38349\n",
            "19189\n",
            "75147\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1dca564afc0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"faces/0_link1.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-030f9cbc2ee0>\u001b[0m in \u001b[0;36mverify\u001b[0;34m(image_path, identity, database, model)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-74b7eb35cec7>\u001b[0m in \u001b[0;36mimg_to_encoding\u001b[0;34m(image_path, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimg_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for key in database:\n",
        "  dist, do = verify(\"faces/0_link1.jpg\", key, database, FRmodel)\n",
        "  if (do==True):\n",
        "    print (key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0Ss8SIk6gUB",
        "outputId": "683ce6e4-168d-4c9f-f06d-a349c553db19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.7117669, False)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "verify(\"faces/0_45980.jpg\", \"sanjana\", database, FRmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "H8xRzE1k6nzt",
        "outputId": "1b9f92e6-6234-42a2-91e8-d914f668bfc3"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image('faces/0_45980.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Kx9it_I5989",
        "outputId": "d681ec98-49b5-4233-ea77-8a9f8f4f4d57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.601937, True)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "verify(\"faces/0_38655.jpg\", \"sanjana\", database, FRmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "JW0rE9yl6JVQ",
        "outputId": "e2786139-ce79-48e9-ee2a-977a0b183a6c"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image('faces/0_38655.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AABC5isbra6E",
        "outputId": "9747b327-2b32-4177-b5cf-e5c469f0730d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.43165582, True)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_face(\"train/a.jpg\")\n",
        "verify(\"faces/0_a.jpg\", \"sanjana\", database, FRmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMwXMmrorlyJ",
        "outputId": "c75d9d16-5798-4628-8aff-a54df82205b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.45908517, True)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_face(\"train/b.jpg\")\n",
        "verify(\"faces/0_b.jpg\", \"sanjana\", database, FRmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzkLXtmNrn3n",
        "outputId": "4e5d20da-c3ab-491b-abf5-77f9ccbae0f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.69862276, True)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_face(\"train/c.jpg\")\n",
        "verify(\"faces/0_c.jpg\", \"sanjana\", database, FRmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLA3Cx2drpjd",
        "outputId": "77d91d2a-a9e7-43a8-cb9b-b5e1c55cade0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.41899475, True)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_face(\"train/d.jpg\")\n",
        "verify(\"faces/0_d.jpg\", \"sanjana\", database, FRmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hjms7-7rrRS",
        "outputId": "f2b8fb81-0924-4a46-8c05-922c8a9fb72e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.3997589, True)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_face(\"train/e.jpg\")\n",
        "verify(\"faces/0_e.jpg\", \"sanjana\", database, FRmodel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_L3tPQmuI41"
      },
      "source": [
        "### References:\n",
        "\n",
        "- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
        "- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n",
        "- The pretrained model is inspired by Victor Sy Wang's implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace.\n",
        "- Inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet \n",
        "- Inspired from CS 230 Happy House\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Face Verification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
